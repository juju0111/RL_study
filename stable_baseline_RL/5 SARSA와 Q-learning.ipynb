{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3c63bc7",
   "metadata": {},
   "source": [
    "# SARSA와 Q-learning 응용\n",
    "\n",
    "FrozenLake 환경에 SARSA와 Q-learning 을 적용해 보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f68bad",
   "metadata": {},
   "source": [
    "### SARSA 프로그램 \n",
    "\n",
    "FrozenLake-v0의 환경을 만들고 $Q(s,a) $를 key '(s,a)'를 가진 dictionary 데이터로 정의하고 초기치를 uniform(0,1)으로 부여하고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b20903f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4c5d33c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "env = gym.make('FrozenLake-v1')\n",
    "\n",
    "Q= {}\n",
    "for s in range(env.observation_space.n):\n",
    "    for a in range(env.action_space.n):\n",
    "        Q[(s,a)] = random.uniform(0,1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae0540a",
   "metadata": {},
   "source": [
    "다음은 epsilon-greedy policy 를 정의한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1d0d370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(state,epsilon):\n",
    "    if random.uniform(0,1) < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        return max(list(range(env.action_space.n)),key=lambda x:Q[(state,x)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cd03e5",
   "metadata": {},
   "source": [
    "Q(s,a)를 update하기 위해 학습률 $\\alpha = 0.1$, $discount factor \\gamma=0.9$, 그리고 epsilon-greedy policy를 위해 $\\epsilon = 0.2$로 부여하고 있다. 프로그램에서 볼 수 있듯 $Q(s_{t+1},a_{t+1}) =Q(s\\_,a\\_)$라고 표시하였고 action a_의 선택은 epsilon-greedy policy로 결정하고있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "241d3088",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "gamma = 0.9 # discount factor\n",
    "epsilon = 0.2\n",
    "\n",
    "num_episodes = 200000\n",
    "timesteps = 1000\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    s = env.reset()\n",
    "    ### SARSA\n",
    "    a = epsilon_greedy(s,epsilon)\n",
    "    ###\n",
    "    for t in range(timesteps):\n",
    "        s_,r,done,_ = env.step(a)\n",
    "        \n",
    "        ###  SARSA\n",
    "        a_ = epsilon_greedy(s_,epsilon)\n",
    "        ###\n",
    "        \n",
    "        Q[(s,a)] = Q[(s,a)] + alpha*(r + gamma*Q[(s_,a_)] - Q[(s,a)])\n",
    "        s = s_\n",
    "        a = a_\n",
    "        if done:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f111011",
   "metadata": {},
   "source": [
    "아래 프로그램과 같이 최종 추정된 $Q(s,a)$를 출력하고 있으며\n",
    "\n",
    "state 0에서 최적 policy는 action=1(down)\n",
    "\n",
    "state 1 에서는 action=2(right)임을 보여주고 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1286548b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   state_action_pair     value\n",
      "0             (0, 0)  0.464199\n",
      "1             (0, 1)  0.471603\n",
      "2             (0, 2)  0.478725\n",
      "3             (0, 3)  0.458124\n",
      "4             (1, 0)  0.472069\n",
      "5             (1, 1)  0.522923\n",
      "6             (1, 2)  0.515306\n",
      "7             (1, 3)  0.467932\n",
      "8             (2, 0)  0.539115\n",
      "9             (2, 1)  0.512157\n",
      "10            (2, 2)  0.506227\n",
      "11            (2, 3)  0.504466\n"
     ]
    }
   ],
   "source": [
    "# SARSA\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(list(Q.items()), columns=['state_action_pair', 'value'])\n",
    "print(df.head(12))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b870f2c4",
   "metadata": {},
   "source": [
    "아래 코드는 Q(14,1)이 가장 큰 값을 갖고 있으므로 state = 14에서 action=1 ( down)이 최적 policy인 것을 보여주고 있다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "779c978f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   state_action_pair     value\n",
      "56           (14, 0)  0.810836]\n",
      "[   state_action_pair     value\n",
      "57           (14, 1)  1.026793]\n",
      "[   state_action_pair     value\n",
      "58           (14, 2)  1.216296]\n",
      "[   state_action_pair     value\n",
      "59           (14, 3)  0.882546]\n"
     ]
    }
   ],
   "source": [
    "print([df[df['state_action_pair']==(14,0)]])\n",
    "print([df[df['state_action_pair']==(14,1)]])\n",
    "print([df[df['state_action_pair']==(14,2)]])\n",
    "print([df[df['state_action_pair']==(14,3)]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81eea442",
   "metadata": {},
   "source": [
    "### Q-learning 실행 프로그램 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d5915351",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "env = gym.make('FrozenLake-v1')\n",
    "\n",
    "Q= {}\n",
    "for s in range(env.observation_space.n):\n",
    "    for a in range(env.action_space.n):\n",
    "        Q[(s,a)] = random.uniform(0,1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1d68004e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(state,epsilon):\n",
    "    if random.uniform(0,1) < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        return max(list(range(env.action_space.n)),key=lambda x:Q[(state,x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2c795108",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "gamma = 0.9 # discount factor\n",
    "epsilon = 0.2\n",
    "\n",
    "num_episodes = 200000\n",
    "timesteps = 1000\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    s = env.reset()\n",
    "\n",
    "    for t in range(timesteps):\n",
    "        a = epsilon_greedy(s,epsilon)\n",
    "        s_,r,done,_ = env.step(a)\n",
    "        \n",
    "        ### next Q의 action은 argmax로 결정 \n",
    "        a_ = np.argmax([Q[(s,a)] for  a in range(env.action_space.n)])\n",
    "        ###\n",
    "        \n",
    "        Q[(s,a)] = Q[(s,a)] + alpha*(r + gamma*Q[(s_,a_)] - Q[(s,a)])\n",
    "        s = s_\n",
    "        if done:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1b869c70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   state_action_pair     value\n",
      "0             (0, 0)  0.456220\n",
      "1             (0, 1)  0.484444\n",
      "2             (0, 2)  0.457622\n",
      "3             (0, 3)  0.454794\n",
      "4             (1, 0)  0.502102\n",
      "5             (1, 1)  0.576188\n",
      "6             (1, 2)  0.477732\n",
      "7             (1, 3)  0.460257\n",
      "8             (2, 0)  0.475832\n",
      "9             (2, 1)  0.484957\n",
      "10            (2, 2)  0.476598\n",
      "11            (2, 3)  0.454844\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(list(Q.items()), columns=['state_action_pair','value'])\n",
    "print(df.head(12))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99e21ac",
   "metadata": {},
   "source": [
    "아래 프로그램은 Q(14,1)이 가장 큰 값을 갖고 있으며 state=14에서 action = 1이 최적 policy임을 보여주며, SARSA와 다른 결과를 보여주고있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "35934881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   state_action_pair     value\n",
      "56           (14, 0)  0.791015]\n",
      "[   state_action_pair     value\n",
      "57           (14, 1)  1.294372]\n",
      "[   state_action_pair     value\n",
      "58           (14, 2)  0.985858]\n",
      "[   state_action_pair     value\n",
      "59           (14, 3)  0.975202]\n"
     ]
    }
   ],
   "source": [
    "print([df[df['state_action_pair']==(14,0)]])\n",
    "print([df[df['state_action_pair']==(14,1)]])\n",
    "print([df[df['state_action_pair']==(14,2)]])\n",
    "print([df[df['state_action_pair']==(14,3)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f5bea0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "[vision_project]",
   "language": "python",
   "name": "vision_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
